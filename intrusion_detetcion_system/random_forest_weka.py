# -*- coding: utf-8 -*-
"""random_forest_Weka.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16OxUdon3CrmrTJZhMzb5EaVJ3vd04jnW
"""

import tensorflow as tf
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/weka.csv')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# might be needed depending on your version of Jupyter
# %matplotlib inline

df.info()

df.head()

df.corr()

df.isnull().sum().sum()

df[' Label'].unique()

len(df)

df.select_dtypes(['object']).columns

df.shape

df.describe()

df.transpose()

sns.countplot(x=' Label',data=df);

# converting each Label attacks names into digits
df['label'] = pd.factorize(df[' Label'])[0]

df.head()

df = df.drop(' Label', axis=1)

df.head()

"""# Feature Selection """

df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)

# Instead of dropping rows which contain any nulls and infinite numbers, 
# it is more succinct to the reverse the logic of that and instead 
#return the rows where all cells are finite numbers. 
#The numpy isfinite function does this and the '.all(1)' will only return a 
#TRUE if all cells in row are finite.
df = df[np.isfinite(df).all(1)]

len(df)

from sklearn.model_selection import train_test_split
X = df.drop('label',axis=1).values
y = df['label'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)

X.shape

y.shape

"""Remove Constant, Quasi Constant and Duplicate features"""

from sklearn.feature_selection import VarianceThreshold

constant_filter = VarianceThreshold(threshold=0.01)
constant_filter.fit(X_train)
X_train_filter = constant_filter.transform(X_train)
X_test_filter = constant_filter.transform(X_test)

X_train_filter.shape, X_test_filter.shape

"""Remove duplicate features"""

X_train_T = X_train_filter.T # taking the transpose
X_test_T = X_test_filter.T

X_train_T = pd.DataFrame(X_train_T)
X_test_T = pd.DataFrame(X_test_T)

X_train_T.duplicated().sum()

duplicate_features = X_train_T.duplicated()

features_to_keep = [not index for index in duplicate_features]

X_train_unique = X_train_T[features_to_keep].T
X_test_unique = X_test_T[features_to_keep].T

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(X_train_unique)
X_train_unique = scaler.transform(X_train_unique)
X_test_unique = scaler.transform(X_test_unique)

X_train_unique = pd.DataFrame(X_train_unique)
X_test_unique = pd.DataFrame(X_test_unique)

X_train_unique.shape, X_test_unique.shape

"""Pearson Correlated features removal"""

corrmat = X_train_unique.corr()

plt.figure(figsize=(12,4))
sns.heatmap(corrmat)

corrmat.shape

# Finding the correlated features
def get_correlation(data, threshold):
    corr_col = set() # makes a set of unrepeated data
    corrmat = data.corr() # getting a corr matrix
    for i in range(len(corrmat.columns)):
        for j in range(i): # corr -> (i,j) = (1,0), (1,2), ..., (2,0)......
            if abs(corrmat.iloc[i,j]) > threshold:
                colname = corrmat.columns[i]
                corr_col.add(colname)
    return corr_col
corr_features = get_correlation(X_train_unique, 0.70)
print("Correlated features:", len(set(corr_features)))

corr_features

X_train_uncorr = X_train_unique.drop(labels=corr_features, axis=1)
X_test_uncorr = X_test_unique.drop(labels=corr_features, axis=1)

X_train_uncorr.shape, X_train_unique.shape

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

# Instantiate model with 100 decision trees
rc = RandomForestClassifier(n_estimators = 100, random_state = 101, n_jobs=-1)
# Train the model on training data
rc.fit(X_train_uncorr, y_train)

predictions = rc.predict(X_test_uncorr)

predictions

# viewing the predicted probabilites of first 10 rows of test
rc.predict_proba(X_test_uncorr)[0:10]

# confusion matrix
pd.crosstab(y_test,predictions,rownames=['Actual Attack'], colnames=['Predicted Attack'] )

from sklearn.metrics import classification_report

print(classification_report(y_test,predictions))

